{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2DIYxc8-O8s"
      },
      "outputs": [],
      "source": [
        "# Homework 2: Boosting and Bagging\n",
        "#\n",
        "# This python notebook is for Homework 2 of CS6350 at the University of Utah. It takes the Decision Trees from the previous\n",
        "# homework and builds on them to implement the Boosting and Bagging algorithms.\n",
        "#\n",
        "# @author: Scott Merkley\n",
        "# @version: October 06, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsjurSrH-7xH"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DguBYW3t-tMS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkIwnbj9RfD6"
      },
      "source": [
        "# **Gain Functions, Node Class, and ID3 Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qfa2PSsBRjzl"
      },
      "outputs": [],
      "source": [
        "####################################################################################################################################################################################################################################\n",
        "#                                                                                                                                                                                                                                  #\n",
        "#                                                                                                 Entropy and Gain Functions                                                                                                       #\n",
        "#                                                                                                                                                                                                                                  #\n",
        "####################################################################################################################################################################################################################################\n",
        "\n",
        "# This function takes in a positive and negative numbers for one attribute of a set, the total\n",
        "# of their entries and calculates the entropy for that single array.\n",
        "#\n",
        "def Calculate_Single_Entropy(arr):\n",
        "  tot = np.sum(arr)\n",
        "  H_entropy = 0\n",
        "  for val in arr:\n",
        "    if val == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      H_entropy += -(val / tot) * math.log2(val / tot)\n",
        "  return H_entropy\n",
        "\n",
        "\n",
        "# This function takes in the positive and negative numbers of chances that the attribute has,\n",
        "# calculates, and returns the entropy.\n",
        "#\n",
        "def Entropy(entropy_arr):\n",
        "\n",
        "  if not isinstance(entropy_arr, np.ndarray):\n",
        "    entropy_arr = np.array(entropy_arr)\n",
        "\n",
        "  try:\n",
        "    if entropy_arr.shape[1] > 1:\n",
        "      H_entropy = 0\n",
        "      for arr in entropy_arr:\n",
        "            H_entropy += np.sum(arr) / np.sum(entropy_arr) * Calculate_Single_Entropy(arr)\n",
        "      return H_entropy\n",
        "  except:\n",
        "      return Calculate_Single_Entropy(entropy_arr)\n",
        "\n",
        "\n",
        "# This function calculates the gain from given positive and negative values in an array so\n",
        "# that the decision tree can decide which attribute to use next.\n",
        "#\n",
        "# Format of 'entropies':\n",
        "#\n",
        "#    [[pos, neg],     First attribute with pos, neg values\n",
        "#     [pos, neg],     Second attribute with pos, neg values\n",
        "#     [pos, neg],\n",
        "#        ...\n",
        "#     [pos, neg]]     Total pos, neg of system\n",
        "#\n",
        "def Gain(df):\n",
        "\n",
        "  # Check if df is a df, if not, make it a df\n",
        "  entropies = []\n",
        "  for col_name in df.columns:\n",
        "    col_entropy = []\n",
        "    for val in pd.crosstab(df[col_name], df[df.columns[-1]]).values:\n",
        "        col_entropy.append(val)\n",
        "\n",
        "    if col_name == df.columns[-1]:\n",
        "      # You need to collate these values so that they are in the form [9, 5] instead of [9, 0], [0, 5]\n",
        "      col_entropy = [max(elements) for elements in zip(*col_entropy)]\n",
        "\n",
        "    # I want to calculate the entropy for each column here\n",
        "    entropies.append(Entropy(col_entropy))\n",
        "\n",
        "  # Calculating the Gain\n",
        "  entropies = entropies[-1] - entropies\n",
        "  entropies = entropies[:-1]\n",
        "  return np.array(entropies)\n",
        "\n",
        "\n",
        "# This function calculates the Majority Error of a given dataframe, it assumes that the first columns are the attributes and the last column is the labels.\n",
        "# It returns an array of errors to find the majority from.\n",
        "#\n",
        "def Majority_Error(df):\n",
        "  errors = []\n",
        "  for col_name in df.columns[:-1]:\n",
        "    errors.append(1 - pd.crosstab(df[col_name], df[df.columns[-1]]).sum(axis = 1).max() / pd.crosstab(df[col_name], df[df.columns[-1]]).values.sum())\n",
        "  return np.array(errors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# This function calculates the Gini Index of a given dataframe, it assumes that the first columns are the attributes and the last column is the labels.\n",
        "# It returns an array of errors to find the Gini from.\n",
        "#\n",
        "def Gini_Index(df):\n",
        "  gini = []\n",
        "  for col_name in df.columns[:-1]:\n",
        "    gini_value = 0\n",
        "    for val in pd.crosstab(df[col_name], df[df.columns[-1]]).values:\n",
        "      gini_value += (np.sum(val) / pd.crosstab(df[col_name], df[df.columns[-1]]).values.sum())**2\n",
        "    gini.append(1 - gini_value)\n",
        "  return np.array(gini)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################################################################################################################################################\n",
        "#                                                                                                                                                                                                                                  #\n",
        "#                                                                                                     Node Class                                                                                                                   #\n",
        "#                                                                                                                                                                                                                                  #\n",
        "####################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "# This class represents the nodes for the decision tree. The data can be thought of as the name of the tree,\n",
        "# the children are the leaf nodes.\n",
        "#\n",
        "class Node:\n",
        "  def __init__(self, data = None):\n",
        "    self.data = data\n",
        "    self.children = {}\n",
        "    self.is_leaf = False\n",
        "\n",
        "  def add_child(self, branch_name, child_node):\n",
        "    self.children[branch_name] = child_node\n",
        "\n",
        "  def __getitem__(self, child_name):\n",
        "    return self.children[child_name]\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data)\n",
        "\n",
        "  def print_tree(self, level = 0):\n",
        "        # Print the current node's data with indentation\n",
        "        print(\" \" * level + str(self.data))\n",
        "        # Recursively print the children nodes\n",
        "        for child in self.children.values():\n",
        "            child.print_tree(level + 2)  # Increases indentation for children\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################################################################################################################################################\n",
        "#                                                                                                                                                                                                                                  #\n",
        "#                                                                                                     ID3 Algorithm                                                                                                                #\n",
        "#                                                                                                                                                                                                                                  #\n",
        "####################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "# This function is the decision tree function. It takes in a given data set S, Attributes (ie. column names without the label column name), and\n",
        "# the Labels for the dataset. It then uses the Node class and Gain functions to create a decision tree for the data set. The user is also able to\n",
        "# specify a depth for the tree to try to maintain overfitting or underfitting.\n",
        "#\n",
        "def ID3(S, Attributes, Labels, method = 'Gain', max_depth = None, current_depth = 0):\n",
        "\n",
        "  if max_depth != None and not isinstance(max_depth, int):\n",
        "    max_depth = int(max_depth)\n",
        "\n",
        "  if not isinstance(Attributes, list):\n",
        "    attributes_list = []\n",
        "    for a in Attributes:\n",
        "      attributes_list.append(a)\n",
        "    Attributes = attributes_list\n",
        "\n",
        "  df = pd.DataFrame(S, columns = Attributes)\n",
        "  outcome_column_name = 'Outcomes'\n",
        "  df[outcome_column_name] = Labels\n",
        "\n",
        "  if len(df[outcome_column_name].unique()) == 1:\n",
        "    leaf = Node(df[outcome_column_name].unique()[0])\n",
        "    leaf.is_leaf = True\n",
        "    return leaf\n",
        "\n",
        "  elif len(Attributes) == 0:\n",
        "    leaf = Node(df[outcome_column_name].value_counts().idxmax())\n",
        "    leaf.is_leaf = True\n",
        "    return leaf # Most common label, not the highest number.\n",
        "\n",
        "  else:\n",
        "    if method == 'ME':\n",
        "      A = Attributes[np.argmax(Majority_Error(df))]\n",
        "    elif method == 'GI':\n",
        "      A = Attributes[np.argmax(Gini_Index(df))]\n",
        "    else:\n",
        "      A = Attributes[np.argmax(Gain(df))] # Will return the 'best' attribute (aka. column header) that splits S\n",
        "\n",
        "    # Add a new node until the specified depth\n",
        "    root = Node(A)\n",
        "\n",
        "    if max_depth == None or current_depth < max_depth:\n",
        "      for v in df[A].unique():\n",
        "        # root.add(Node(v)) # Adding a new tree branch corresponding to A = v\n",
        "        S_v = df[df[A] == v]\n",
        "\n",
        "        if S_v.empty:\n",
        "          leaf = Node(df[outcome_column_name].value_counts().idxmax())\n",
        "          leaf.is_leaf = True\n",
        "          root.add_child(v, leaf)\n",
        "\n",
        "        else:\n",
        "          # In order to make the tree a certain depth, you could use a count depth on the node class! Then instead of running a leaf you only add a branch (aka. .add())\n",
        "          L_v = S_v[outcome_column_name].values\n",
        "          S_v = S_v.drop([A, outcome_column_name], axis = 1)\n",
        "          A_v = Attributes.copy()\n",
        "          A_v.remove(A)\n",
        "          root.add_child(v, ID3(S_v.values, A_v, L_v, method = method, max_depth = max_depth, current_depth = current_depth + 1))\n",
        "\n",
        "    else:\n",
        "      leaf = Node(df[outcome_column_name].value_counts().idxmax())\n",
        "      leaf.is_leaf = True\n",
        "      return leaf\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "####################################################################################################################################################################################################################################\n",
        "#                                                                                                                                                                                                                                  #\n",
        "#                                                                                                    Testing Decision Tree Functions                                                                                               #\n",
        "#                                                                                                                                                                                                                                  #\n",
        "####################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "# This function takes in a decision tree, the test data (as a pandas dataframe), and the row index. It returns the label value that the decision tree returns.\n",
        "# The function recursively moves down the tree until a leaf node is reached. The leaf nodes value is then returned up the tree.\n",
        "#\n",
        "def Test_Tree(tree, test, row_index):\n",
        "  if tree.is_leaf:\n",
        "    return tree.data\n",
        "  else:\n",
        "    return Test_Tree(tree[test[tree.data][row_index]], test, row_index)\n",
        "\n",
        "\n",
        "\n",
        "# This function is the driver method for testing the a decision tree. An input tree, test dataframe, outcome column name, and beginning and ending depths\n",
        "# are required to make the calculations. It then traverses the tree, tests whether the outcomes in the decision tree are the same as the test data.\n",
        "#\n",
        "def Test_Decision_Tree(tree, test_df, outcome_column_name, begin_depth, end_depth):\n",
        "  correct_dictionary = {}\n",
        "  for m in ['ME', 'GI', 'Gain']:\n",
        "    correct_list = []\n",
        "    for depth in range(begin_depth, end_depth):\n",
        "      tree = ID3(test_df.drop(outcome_column_name, axis = 1).values, list(test_df.drop(outcome_column_name, axis = 1).columns), test_df[outcome_column_name].values, method = m, max_depth = depth)\n",
        "      is_correct = 0\n",
        "      for row_index in range(0, len(test_df)):\n",
        "        try:\n",
        "          if Test_Tree(tree, test_df, row_index) == test_df[outcome_column_name][row_index]:\n",
        "            is_correct += 1\n",
        "        except:\n",
        "          is_correct += 0\n",
        "      correct_list.append(is_correct)\n",
        "    correct_dictionary[m] = correct_list\n",
        "  return correct_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI_6TjtJdokO"
      },
      "source": [
        "# **AdaBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x9VSApDpGBxK"
      },
      "outputs": [],
      "source": [
        "####################################################################################################################################################################################################################################\n",
        "#                                                                                                                                                                                                                                  #\n",
        "#                                                                                                          Class for Adaboost                                                                                                      #\n",
        "#                                                                                                                                                                                                                                  #\n",
        "####################################################################################################################################################################################################################################\n",
        "\n",
        "class AdaBoost:\n",
        "  def __init__(self, t_stumps = 0):\n",
        "    self.t_stumps = t_stumps\n",
        "    self.alphas = []\n",
        "    self.stumps = []\n",
        "\n",
        "  def fit(self, S, Attributes, Labels):\n",
        "    # Check and see if the provided labels are [-1, 1]\n",
        "    if not (np.unique(Labels) == np.array([-1, 1])).all:\n",
        "      raise Exception(\"Labels must be -1 or 1\")\n",
        "\n",
        "    df = pd.DataFrame(S, columns = Attributes)\n",
        "    S_i = S\n",
        "\n",
        "    # Initialize Weights\n",
        "    weights = np.ones(S.shape[0]) / S.shape[0]\n",
        "\n",
        "    # Train the stumps\n",
        "    for s in range(self.t_stumps):\n",
        "\n",
        "      # Get a subset of S that is S_i\n",
        "      cumulative_weights_for_indexing = np.cumsum(weights)\n",
        "      indices = []\n",
        "      for j in range(S_i.shape[0]): # Could take a subset of 20% of S (in the video they say to take the samples as the same number as was given aka. 100%)\n",
        "        indices.append(np.searchsorted(cumulative_weights_for_indexing, np.random.rand()))\n",
        "      S_i = S[indices]\n",
        "      Labels_i = Labels[indices]\n",
        "\n",
        "      # Train the individeual stump, Attributes is just the columns of S, depth of a stump is 1 by definition\n",
        "      stump = ID3(S_i, Attributes, Labels_i, method = 'Gain', max_depth = 1)\n",
        "\n",
        "      # Calculate the error using the weight\n",
        "      predictions = np.array([stump[pd.DataFrame(S_i, columns = Attributes)[stump.data][i]].data for i in range(S_i.shape[0])])\n",
        "      incorrect = (predictions != Labels_i).astype(int)\n",
        "      weighted_errors = np.sum(weights * incorrect) / np.sum(weights)\n",
        "\n",
        "      # Calculate the weight of the stump\n",
        "      if weighted_errors == 0:\n",
        "        alpha = 1\n",
        "      else:\n",
        "        alpha = 0.5 * np.log((1 - weighted_errors) / (weighted_errors + 1e-10)) # adding a small term so that you never divide by 0\n",
        "\n",
        "      self.alphas.append(alpha)\n",
        "      self.stumps.append(stump) # Try doing a dictionary and pulling the specific stump you need? {}\n",
        "\n",
        "      # Update the Weights\n",
        "      weights *= np.exp(-alpha * Labels_i.astype(int) * incorrect)\n",
        "      weights /= np.sum(weights) # Normalize Weights\n",
        "\n",
        "  def predict(self, S):\n",
        "    # Find the predictions for a given dataset using alphas\n",
        "    final_predictions = np.zeros(S.shape[0])\n",
        "    for alpha, stump in zip(self.alphas, self.stumps):\n",
        "      predictions = np.array([stump[S[stump.data][i]].data for i in range(S.shape[0])])\n",
        "      final_predictions += alpha * predictions\n",
        "\n",
        "    # Lastly, return the signs of the predictions\n",
        "    return np.sign(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brpCLz-6Ita-"
      },
      "source": [
        "# **AdaBoost: Bank Marketing Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Trr0JJbUIst5",
        "outputId": "05565ee8-d90c-4116-af23-a2c2809b7981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 0\n",
            "5 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e3e23720408a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumber_of_trees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mbank_learner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_stumps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mbank_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbank_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbank_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbank_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_test_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbank_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ff46fb8952f2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, S, Attributes, Labels)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m# Calculate the error using the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstump\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstump\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mincorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mLabels_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mweighted_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mincorrect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ff46fb8952f2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m# Calculate the error using the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstump\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstump\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mincorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mLabels_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mweighted_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mincorrect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m# TODO: What about re-joining object columns?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             block_values = [\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvals_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvals_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m# TODO: What about re-joining object columns?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             block_values = [\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvals_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvals_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             ]\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_block_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDatetimeTZBlock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mget_block_type\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;31m# Need this first(ish) so that Sparse[datetime] is sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtensionBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/base.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    276\u001b[0m            \u001b[0mconditions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \"\"\"\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Downloading Car Evaluation CSV from website so I dont have to store it locally.\n",
        "columns = [\n",
        "    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
        "    'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
        "    'previous', 'poutcome', 'y'\n",
        "]\n",
        "\n",
        "bank_data = pd.read_csv('bank/train.csv', names = columns)\n",
        "bank_test_data = pd.read_csv('bank/test.csv', names = columns)\n",
        "\n",
        "# Change all these to binary using threshold age, balance, day, duration, days, previous, campaign\n",
        "for column_name in ['age', 'balance', 'day', 'duration', 'pdays', 'previous', 'campaign']:\n",
        "  bank_test_data[column_name] = bank_test_data[column_name] > bank_data[column_name].median()\n",
        "  bank_data[column_name] = bank_data[column_name] > bank_data[column_name].median()\n",
        "\n",
        "# Create Decision Tree from Dataset\n",
        "# bank_tree = ID3(bank_data.drop('y', axis = 1).values, list(bank_data.drop('y', axis = 1).columns), bank_data['y'].values, method = 'Gain', max_depth = 1)\n",
        "# draw_dot(bank_tree)\n",
        "\n",
        "\n",
        "# Change the Labels to 1 or -1\n",
        "bank_data['y'] = bank_data['y'].replace({'yes' : 1, 'no' : -1})\n",
        "\n",
        "\n",
        "number_of_trees = [1, 5, 10, 50, 100, 250, 500]\n",
        "for n in number_of_trees:\n",
        "  bank_learner = AdaBoost(t_stumps = n)\n",
        "  bank_learner.fit(bank_data.drop('y', axis = 1).values, bank_data.drop('y', axis = 1).columns.tolist(), bank_data['y'].values)\n",
        "  b = bank_learner.predict(bank_test_data) == bank_test_data['y'].values\n",
        "\n",
        "  print(n, np.sum(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYt4h0nMdwwM"
      },
      "source": [
        "# **Bagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v8hHDlKf3ge-"
      },
      "outputs": [],
      "source": [
        "####################################################################################################################################################################################################################################\n",
        "#                                                                                                                                                                                                                                  #\n",
        "#                                                                                                        Class for RandomForest                                                                                                    #\n",
        "#                                                                                                                                                                                                                                  #\n",
        "####################################################################################################################################################################################################################################\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "\n",
        "\n",
        "class RandomForest:\n",
        "  def __init__(self, n_trees = 0):\n",
        "    self.n_trees = n_trees\n",
        "    self.forest = []\n",
        "\n",
        "  def Train_Trees_MP(self, args):\n",
        "      S, Attributes, Labels = args\n",
        "      return ID3(S, Attributes, Labels, method = 'Gain')\n",
        "\n",
        "  def fit(self, S, Attributes, Labels):\n",
        "    df = pd.DataFrame(S, columns = Attributes)\n",
        "    S_i = S\n",
        "\n",
        "    # Initialize Weights\n",
        "    weights = np.ones(S.shape[0]) / S.shape[0]\n",
        "\n",
        "    # Make a list to give to the multiprocessing\n",
        "    args = []\n",
        "\n",
        "    # Train the stumps\n",
        "    for t in range(self.n_trees):\n",
        "      # Get a subset of S that is S_i\n",
        "      cumulative_weights_for_indexing = np.cumsum(weights)\n",
        "      indices = []\n",
        "      for j in range(np.ceil(S_i.shape[0] * 0.25).astype(int)): # Could take a subset of 20% of S\n",
        "        indices.append(np.random.randint(S.shape[0]))\n",
        "      S_i = S[indices]\n",
        "      Labels_i = Labels[indices]\n",
        "      args.append((S_i, Attributes, Labels_i))\n",
        "\n",
        "    # Run Multiprocessing\n",
        "    with mp.Pool(mp.cpu_count()) as pool:\n",
        "      # Train the individual stump, Attributes is just the columns of S, depth of a stump is 1 by definition\n",
        "      trees = pool.map(self.Train_Trees_MP, args)\n",
        "\n",
        "    # Add all the trees to the forest\n",
        "    self.forest.extend(trees)\n",
        "\n",
        "  def predict(self, S):\n",
        "    # Find the predictions for a given dataset using alphas\n",
        "    final_predictions = []\n",
        "    for i in range(S.shape[0]):\n",
        "      for t in self.forest:\n",
        "        try:\n",
        "          # Add the random tree if it works, if it doesn't, continue onto the next tree until you find one that works\n",
        "          tree = Test_Tree(t, S, i)\n",
        "          final_predictions.append(tree)\n",
        "          break\n",
        "        except:\n",
        "          if t == self.forest[-1]:\n",
        "            try:\n",
        "              final_predictions.append(S.iloc[:, -1].mode()[0]) # Just give it the most common value\n",
        "            except:\n",
        "              final_predictions.append('NA') # Just give it a wrong value\n",
        "\n",
        "    # Lastly, return the signs of the predictions\n",
        "    return final_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "y5EZf8VR3pgl",
        "outputId": "40714875-c7d6-48bd-b266-22821556fbc6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ef7be204e3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumber_of_trees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mbank_learner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mbank_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbank_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbank_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbank_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_test_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbank_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mcorrect_over_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9807ed370461>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, S, Attributes, Labels)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;31m# Train the individual stump, Attributes is just the columns of S, depth of a stump is 1 by definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_Trees_MP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Add all the trees to the forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Downloading Car Evaluation CSV from website so I dont have to store it locally.\n",
        "columns = [\n",
        "    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
        "    'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
        "    'previous', 'poutcome', 'y'\n",
        "]\n",
        "\n",
        "bank_data = pd.read_csv('bank/train.csv', names = columns)\n",
        "bank_test_data = pd.read_csv('bank/test.csv', names = columns)\n",
        "\n",
        "# Change all these to binary using threshold age, balance, day, duration, days, previous, campaign\n",
        "for column_name in ['age', 'balance', 'day', 'duration', 'pdays', 'previous', 'campaign']:\n",
        "  bank_test_data[column_name] = bank_test_data[column_name] > bank_data[column_name].median()\n",
        "  bank_data[column_name] = bank_data[column_name] > bank_data[column_name].median()\n",
        "\n",
        "\n",
        "correct_over_time = []\n",
        "number_of_trees = [1, 5, 10, 50, 100, 250, 500]\n",
        "for n in number_of_trees:\n",
        "  bank_learner = RandomForest(n_trees = n)\n",
        "  bank_learner.fit(bank_data.drop('y', axis = 1).values, bank_data.drop('y', axis = 1).columns.tolist(), bank_data['y'].values)\n",
        "  b = bank_learner.predict(bank_test_data) == bank_test_data['y'].values\n",
        "  correct_over_time.append(np.sum(b) / len(bank_test_data['y'].values) * 100)\n",
        "\n",
        "plt.plot(number_of_trees, correct_over_time)\n",
        "plt.title(f'Random Forest Changing Number of Trees')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('% Correct')\n",
        "plt.grid(color = 'lightgrey')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "68oqg7FTfzLX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
